{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mlp.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["jv049ulr6wxE"]}},"cells":[{"metadata":{"id":"AiWokO5On8Rr","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from tensorflow.examples.tutorials.mnist import input_data\n","\n","mnist = input_data.read_data_sets('/tmp/mnist', one_hot=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3GTOW509quc0","colab_type":"code","colab":{}},"cell_type":"code","source":["num_row, num_col = 1, 10\n","f,subplots = plt.subplots(num_row, num_col, sharex='col', sharey='row')\n","\n","X,y = mnist.train.images, mnist.train.labels\n","X = np.reshape(X,(-1,28,28))\n","\n","for i in range(num_col):\n","    X_img = X[np.argmax(y,axis=1) == i].reshape((-1,28,28))\n","    idx = np.random.choice(np.arange(0, X_img.shape[0]))\n","    subplots[i].imshow(X_img[idx], cmap='gray', interpolation='nearest', aspect='auto')\n","    title = 'Digit {}'.format(i)\n","    subplots[i].set_title(title, fontweight=\"bold\")\n","    subplots[i].grid(b=False)\n","    subplots[i].axis('off')\n","\n","f.set_size_inches(18.5, 4.5)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ec9ZoLWHqL09","colab_type":"code","colab":{}},"cell_type":"code","source":["# Placeholders\n","x = tf.placeholder(dtype=tf.float32, shape=[None, 784])     # input placeholder\n","\n","# Placeholder for targets\n","targets = tf.placeholder(dtype=tf.float32, shape=[None, 10])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"X4XwOiaBoJhG","colab_type":"code","colab":{}},"cell_type":"code","source":["def inference(x):\n","\n","  input_dim       = 784\n","  n_classes       = 10\n","  n_hidden        = 256\n","\n","  with tf.variable_scope('network'):\n","      \"\"\"\n","      Write HERE your multi layer perceptron, with one hidden layer \n","      characterised by n_hidden neurons. Note that the last layer\n","      should be followed by a softmax activation, the latter giving \n","      a conditional distribution across n_classes. \n","      \"\"\"\n","      return y\n","    \n","# Define model output\n","y = inference(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jv049ulr6wxE","colab_type":"text"},"cell_type":"markdown","source":["# My solution"]},{"metadata":{"id":"udEgtxEI6uOs","colab_type":"code","colab":{}},"cell_type":"code","source":["def inference(x):\n","\n","  input_dim       = 784\n","  n_classes       = 10\n","  n_hidden        = 256\n","\n","  with tf.variable_scope('network'):\n","      \n","      W1 = tf.Variable(initial_value=tf.random_normal(shape=[input_dim, n_hidden]), name='weights1')\n","      b1 = tf.Variable(initial_value=tf.zeros(shape=[n_hidden]), name='biases1')\n","      \n","      W2 = tf.Variable(initial_value=tf.random_normal(shape=[n_hidden, n_classes]), name='weights2')\n","      b2 = tf.Variable(initial_value=tf.zeros(shape=[n_classes]), name='biases2')\n","  \n","      x1 = tf.matmul(x, W1) + b1\n","      x1 = tf.nn.sigmoid(x1)\n","      \n","      x2 = tf.matmul(x1, W2) + b2\n","      y = tf.nn.softmax(x2)\n","\n","      return y\n","    \n","# Define model output\n","y = inference(x)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"DWjW2vGU62sz","colab_type":"text"},"cell_type":"markdown","source":["# Training Procedure"]},{"metadata":{"id":"sBH4vJ3Io1X_","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define loss function\n","loss = tf.reduce_mean(-tf.reduce_sum(targets * tf.log(y + np.finfo('float32').eps), axis=1))\n","\n","# Define train step\n","train_step = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n","\n","init_op = tf.global_variables_initializer()\n","\n","# Define metrics\n","correct_predictions = tf.equal(tf.argmax(y, axis=1), tf.argmax(targets, axis=1))\n","accuracy = tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n","\n","with tf.Session() as sess:\n","\n","    # Initialize all variables\n","    sess.run(init_op)\n","\n","    # Training parameters\n","    training_epochs = 20\n","    batch_size      = 128\n","\n","    # Number of batches to process to see whole dataset\n","    batches_each_epoch = mnist.train.num_examples // batch_size\n","\n","    for epoch in range(training_epochs):\n","\n","        # During training measure accuracy on validation set to have an idea of what's happening\n","        val_accuracy = sess.run(fetches=accuracy,\n","                                feed_dict={x: mnist.validation.images, targets: mnist.validation.labels})\n","        print('Epoch: {:06d} - VAL accuracy: {:.03f}'.format(epoch, val_accuracy))\n","\n","        for _ in range(batches_each_epoch):\n","\n","            # Load a batch of training data\n","            x_batch, target_batch = mnist.train.next_batch(batch_size)\n","\n","            # Actually run one training step here\n","            sess.run(fetches=[train_step],\n","                     feed_dict={x: x_batch, targets: target_batch})\n","\n","#     # Eventually evaluate on whole test set when training ends\n","#     test_accuracy = sess.run(fetches=accuracy,\n","#                              feed_dict={x: mnist.test.images, targets: mnist.test.labels})\n","#     print('*' * 50)\n","#     print('Training ended. TEST accuracy: {:.03f}'.format(test_accuracy))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TpgU7rZaok3R","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}