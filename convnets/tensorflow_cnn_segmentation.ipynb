{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_cnn_segmentation.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "gHY1Ql6XGqaV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These lines are for making tensorboard visualization work within the iPython notebook environment. "
      ]
    },
    {
      "metadata": {
        "id": "B0ij8mXS1tQ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip -o ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "# Start Tensorboard server\n",
        "LOG_DIR = '/tmp/logs'\n",
        "get_ipython().system_raw('rm -rf {}'.format(LOG_DIR))\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gp_O7spqGune",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Print the public url in which we can find tensorboard."
      ]
    },
    {
      "metadata": {
        "id": "4DviKpvN1xzi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! curl -s http://localhost:4040/api/tunnels | python -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vm4D9ODqOEBe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Install googledrivedownloader."
      ]
    },
    {
      "metadata": {
        "id": "eTjKoVkNCOc8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install googledrivedownloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1A8DswRWG2_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import packages as usual, download the dataset."
      ]
    },
    {
      "metadata": {
        "id": "EaUg0M6OCE1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pickle\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os.path as path\n",
        "from time import time\n",
        "\n",
        "from google_drive_downloader import GoogleDriveDownloader\n",
        "\n",
        "# Download tiles data\n",
        "GoogleDriveDownloader.download_file_from_google_drive(file_id='1W58D4qVZtUAFprDdoC9KRyBWT0k0ie5r',\n",
        "                                                      dest_path='./tiles.zip',\n",
        "                                                      overwrite=True,\n",
        "                                                      unzip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yYfkwkyGIS7W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This class models the Tiles dataset for segmentation of images. Once instantiated, use its members train_x, train_y, validation_x, validation_y, test_x, test_y."
      ]
    },
    {
      "metadata": {
        "id": "Dk5jBqRnCNC2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TilesDataset:\n",
        "\n",
        "    def __init__(self, dataset_root):\n",
        "\n",
        "        self.dataset_root = dataset_root\n",
        "\n",
        "        # Store locations of train, val and test directories\n",
        "        self.train_x_dir      = path.join(dataset_root, 'X_train')\n",
        "        self.train_y_dir      = path.join(dataset_root, 'Y_train')\n",
        "        self.validation_x_dir = path.join(dataset_root, 'X_validation')\n",
        "        self.validation_y_dir = path.join(dataset_root, 'Y_validation')\n",
        "        self.test_x_dir       = path.join(dataset_root, 'X_test')\n",
        "        self.test_y_dir       = path.join(dataset_root, 'Y_test')\n",
        "\n",
        "        # Number of dataset examples\n",
        "        self.train_num_examples      = 10000\n",
        "        self.validation_num_examples = 1000\n",
        "        self.test_num_examples       = 1000\n",
        "\n",
        "        # Initialize empty structures to contain data\n",
        "        self.train_x      = []\n",
        "        self.train_y      = []\n",
        "        self.validation_x = []\n",
        "        self.validation_y = []\n",
        "        self.test_x       = []\n",
        "        self.test_y       = []\n",
        "\n",
        "        # Load images from `dataset_root`\n",
        "        self._fill_data_arrays()\n",
        "\n",
        "    def _fill_data_arrays(self):\n",
        "\n",
        "        # Load training images\n",
        "        for i in range(1, self.train_num_examples + 1):\n",
        "            print('Loading training examples. {} / {}...'.format(i, self.train_num_examples))\n",
        "            x_image = cv2.imread(path.join(self.train_x_dir, '{:05d}.png'.format(i)))\n",
        "            y_image = cv2.imread(path.join(self.train_y_dir, '{:05d}.png'.format(i)), cv2.IMREAD_GRAYSCALE)\n",
        "            self.train_x.append(x_image.astype(np.float32))\n",
        "            self.train_y.append(np.expand_dims(y_image.astype(np.float32), 2))\n",
        "\n",
        "        # Load validation examples\n",
        "        for i in range(1, self.validation_num_examples + 1):\n",
        "            print('Loading validation examples. {} / {}...'.format(i, self.validation_num_examples))\n",
        "            x_image = cv2.imread(path.join(self.validation_x_dir, '{:05d}.png'.format(i)))\n",
        "            y_image = cv2.imread(path.join(self.validation_y_dir, '{:05d}.png'.format(i)), cv2.IMREAD_GRAYSCALE)\n",
        "            self.validation_x.append(x_image.astype(np.float32))\n",
        "            self.validation_y.append(np.expand_dims(y_image.astype(np.float32), 2))\n",
        "\n",
        "        # Load test examples\n",
        "        for i in range(1, self.test_num_examples + 1):\n",
        "            print('Loading test examples. {} / {}...'.format(i, self.test_num_examples))\n",
        "            x_image = cv2.imread(path.join(self.test_x_dir, '{:05d}.png'.format(i)))\n",
        "            y_image = cv2.imread(path.join(self.test_y_dir, '{:05d}.png'.format(i)), cv2.IMREAD_GRAYSCALE)\n",
        "            self.test_x.append(x_image.astype(np.float32))\n",
        "            self.test_y.append(np.expand_dims(y_image.astype(np.float32), 2))\n",
        "\n",
        "    def dump_to_file(self, file_path, protocol=pickle.HIGHEST_PROTOCOL):\n",
        "        with open(file_path, 'wb') as f:\n",
        "            pickle.dump(self, f, protocol=protocol)\n",
        "\n",
        "\n",
        "def convert_target_to_one_hot(target_batch):\n",
        "    \"\"\"\n",
        "    Convert a batch of targets from (height,width,1) to (height,width,2) one-hot encoding.\n",
        "    \"\"\"\n",
        "    b, h, w, c = target_batch.shape\n",
        "    out_tensor = np.zeros(shape=(b, h, w, 2))\n",
        "    for k, cur_example in enumerate(target_batch):\n",
        "        foreground_mask = np.squeeze(cur_example > 0)\n",
        "        background_mask = np.squeeze(cur_example == 0)\n",
        "        out_tensor[k, background_mask, 0] = 1.0\n",
        "        out_tensor[k, foreground_mask, 1] = 1.0\n",
        "    return out_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BiapFCjKJwsT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This class implements the deep model for tiles segmentation. Implement all make_* methods (except for summaries)."
      ]
    },
    {
      "metadata": {
        "id": "pKK4A9oKCmRV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TileSegmenter:\n",
        "\n",
        "    def __init__(self, x, targets, data_shape):\n",
        "\n",
        "        self.x = x\n",
        "        self.targets = targets\n",
        "        self.data_shape = data_shape\n",
        "\n",
        "        self.inference = None\n",
        "        self.loss = None\n",
        "        self.train_step = None\n",
        "        self.summaries = None\n",
        "\n",
        "        self.make_inference()\n",
        "        self.make_loss()\n",
        "        self.make_train_step()\n",
        "        self.make_summaries()\n",
        "\n",
        "    def make_inference(self):\n",
        "      \n",
        "        # Use 2D conv with strides. Stack enough layers to reduce dimensions a bit, \n",
        "        # then upsample using tf.image.resize_bilinear, then apply a final conv to reach 2\n",
        "        # channel outputs.\n",
        "        h, w, c = self.data_shape \n",
        "        \n",
        "        conv1a = tf.layers.conv2d(self.x, filters=32, kernel_size=(3, 3), strides=(2, 2), padding='same', activation=tf.nn.relu)\n",
        "        conv1b = tf.layers.conv2d(conv1a, filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=tf.nn.relu)\n",
        "        conv1c = tf.layers.conv2d(conv1b, filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation=tf.nn.relu)\n",
        "        \n",
        "        conv2a = tf.layers.conv2d(conv1c, filters=64, kernel_size=(3, 3),  strides=(2, 2), padding='same', activation=tf.nn.relu)\n",
        "        conv2b = tf.layers.conv2d(conv2a, filters=64, kernel_size=(3, 3),  strides=(1, 1), padding='same', activation=tf.nn.relu)\n",
        "        conv2c = tf.layers.conv2d(conv2b, filters=64, kernel_size=(3, 3),  strides=(1, 1), padding='same', activation=tf.nn.relu)\n",
        "        \n",
        "        conv3a = tf.layers.conv2d(conv2c, filters=128, kernel_size=(3, 3),  strides=(2, 2), padding='same', activation=tf.nn.relu)\n",
        "        conv3b = tf.layers.conv2d(conv3a, filters=128, kernel_size=(3, 3),  strides=(1, 1), padding='same', activation=tf.nn.relu)\n",
        "        conv3c = tf.layers.conv2d(conv3b, filters=128, kernel_size=(3, 3),  strides=(1, 1), padding='same', activation=tf.nn.relu)\n",
        "        \n",
        "        conv3_up = tf.image.resize_bilinear(conv3c, (64, 64))\n",
        "        \n",
        "        self.inference = tf.layers.conv2d(conv3_up, filters=2, kernel_size=(1, 1), activation=tf.nn.softmax)\n",
        "\n",
        "    def make_loss(self):\n",
        "        # Define loss function\n",
        "        self.loss = - tf.reduce_mean(self.targets * tf.log(self.inference + epsilon))\n",
        "\n",
        "    def make_train_step(self):\n",
        "        self.train_step = tf.train.AdamOptimizer(0.0001).minimize(self.loss)\n",
        "\n",
        "    def make_summaries(self):\n",
        "        \n",
        "        # Add TensorBoard Summaries\n",
        "        how_many_images = 3\n",
        "\n",
        "        # --- scalar summaries\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        tf.summary.image('input', self.x, max_outputs=how_many_images)\n",
        "\n",
        "        # --- foreground image summaries\n",
        "        fg_target_image = tf.expand_dims(tf.gather(tf.transpose(self.targets, [3, 0, 1, 2]), 1), axis=3)\n",
        "        fg_pred_image = tf.expand_dims(tf.gather(tf.transpose(self.inference, [3, 0, 1, 2]), 1), axis=3)\n",
        "        fg_pred_image_rounded = tf.round(tf.nn.softmax(self.inference, dim=3))\n",
        "        fg_pred_image_rounded = tf.expand_dims(tf.gather(tf.transpose(fg_pred_image_rounded, [3, 0, 1, 2]), 1), axis=3)\n",
        "\n",
        "        tf.summary.image('FOREGROUND_(targets)', fg_target_image, max_outputs=how_many_images)\n",
        "        tf.summary.image('FOREGROUND_(prediction)', fg_pred_image, max_outputs=how_many_images)\n",
        "        tf.summary.image('FOREGROUND_ROUNDED_(prediction)', fg_pred_image_rounded, max_outputs=how_many_images)\n",
        "\n",
        "        # --- merge all summaries and initialize the summary writer\n",
        "        self.summaries = tf.summary.merge_all()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RoF4f3ItLvWI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Some parameters..."
      ]
    },
    {
      "metadata": {
        "id": "920WDCJmDNoa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "h, w, c = 64, 64, 3\n",
        "training_epochs = 1000\n",
        "batch_size = 8\n",
        "epsilon = np.finfo(np.float32).eps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GR5myAGmLxKJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instantiate the dataset..."
      ]
    },
    {
      "metadata": {
        "id": "4u3viXOJCziE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load tiles dataset\n",
        "tiles_dataset = TilesDataset(dataset_root='./toy_dataset_tiles')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BAsH8Q4vL0Iv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Placeholders"
      ]
    },
    {
      "metadata": {
        "id": "2KkvX-fHC22p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Placeholders\n",
        "x = tf.placeholder(tf.float32, shape=(None, h, w, c))\n",
        "targets = tf.placeholder(tf.float32, shape=(None, h, w, 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nm2boNYcL2LC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instantiate the model"
      ]
    },
    {
      "metadata": {
        "id": "tnTjpsFZC7HE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "model = TileSegmenter(x, targets, data_shape=(h, w, c))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L-vH9EskL5mI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Start session and initialize variables."
      ]
    },
    {
      "metadata": {
        "id": "q6b1dud0DS5q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = tf.Session()\n",
        "        \n",
        "# Initialize all variables\n",
        "sess.run(tf.global_variables_initializer())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X0ShZkQFMYbV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set up the writer for tensorboard logs."
      ]
    },
    {
      "metadata": {
        "id": "5vMJRFXJDgGi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# FileWriter to save Tensorboard summary\n",
        "train_writer = tf.summary.FileWriter(LOG_DIR, graph=sess.graph)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eVqZzm_PMvQh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training loop, with summaries!"
      ]
    },
    {
      "metadata": {
        "id": "5tVpMTsxDjuz",
        "colab_type": "code",
        "outputId": "2be2a6d7-4b39-432a-aba1-430720f7f529",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Number of batches to process to see whole dataset\n",
        "batches_each_epoch = tiles_dataset.train_num_examples // batch_size\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "\n",
        "    epoch_loss = 0.0\n",
        "\n",
        "    idx_start = 0\n",
        "    for _ in range(batches_each_epoch):\n",
        "\n",
        "        idx_end = idx_start + batch_size\n",
        "\n",
        "        # Load a batch of training data\n",
        "        x_batch = np.array(tiles_dataset.train_x[idx_start:idx_end])\n",
        "        target_batch = np.array(tiles_dataset.train_y[idx_start:idx_end])\n",
        "\n",
        "        # Convert the target batch into one-hot encoding (from 64x64x1 to 64x64x2)\n",
        "        target_batch = convert_target_to_one_hot(target_batch)\n",
        "        \n",
        "        # Preprocess train batch\n",
        "        x_batch -= 128.0\n",
        "\n",
        "        # Actually run one training step here\n",
        "        _, cur_loss = sess.run([model.train_step, model.loss], feed_dict={\n",
        "            x: x_batch,\n",
        "            targets: target_batch\n",
        "        })\n",
        "\n",
        "        idx_start = idx_end\n",
        "\n",
        "        epoch_loss += cur_loss\n",
        "    \n",
        "    # Get summaries for the last batch\n",
        "    summaries = sess.run(model.summaries, feed_dict={\n",
        "            x: x_batch,\n",
        "            targets: target_batch\n",
        "        })\n",
        "    train_writer.add_summary(summaries, epoch)\n",
        "\n",
        "    print('Epoch: {:03d} - Loss: {:.02f}'.format(epoch, epoch_loss / batches_each_epoch))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 088 - Loss: 0.00\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}