{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sequence_counter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "meRdpqkCyCAA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Import proper packages"
      ]
    },
    {
      "metadata": {
        "id": "-0MsSZCyvMJD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import argparse\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from os import makedirs\n",
        "from os.path import exists\n",
        "from os.path import dirname\n",
        "from random import shuffle\n",
        "\n",
        "epsilon = np.finfo(float).eps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HNMAxPJbyFQX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This class models the dataset used for today's lecture: each example is composed of binary sequences, and the target variable encodes how many \"1\" there are within each sequence."
      ]
    },
    {
      "metadata": {
        "id": "M7FNirv1vYhH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SyntheticSequenceDataset:\n",
        "\n",
        "    def __init__(self, max_sequence_length=10, dataset_cache='/tmp/synthetic_dataset.pickle', force_recompute=True):\n",
        "\n",
        "        self._data = None\n",
        "        self.max_sequence_length = max_sequence_length\n",
        "        self.force_recompute = force_recompute\n",
        "        self.dataset_cache = dataset_cache\n",
        "\n",
        "    @property\n",
        "    def data(self):\n",
        "\n",
        "        if not self._data:\n",
        "            if not self.force_recompute and exists(self.dataset_cache):\n",
        "                print('Loading dataset from cache...')\n",
        "                with open(self.dataset_cache, 'rb') as dump_file:\n",
        "                    dataset = pickle.load(dump_file)\n",
        "            else:\n",
        "                print('Recomputing dataset...')\n",
        "                dataset = self._compute_dataset()\n",
        "                if not exists(dirname(self.dataset_cache)):\n",
        "                    makedirs(dirname(self.dataset_cache))\n",
        "                with open(self.dataset_cache, 'wb') as dump_file:\n",
        "                    pickle.dump(dataset, dump_file)\n",
        "\n",
        "            # Store data\n",
        "            self._data = dataset\n",
        "\n",
        "        return self._data\n",
        "\n",
        "    def _compute_dataset(self,):\n",
        "\n",
        "        n = self.max_sequence_length\n",
        "        num_examples = 2 ** n\n",
        "        num_classes = n + 1\n",
        "\n",
        "        # How many examples to use for training (others are for test)\n",
        "        num_train_examples = int(0.8 * num_examples)\n",
        "\n",
        "        # Generate 2**20 binary strings\n",
        "        data_strings = [('{' + '0:0{}b'.format(n) + '}').format(i) for i in range(num_examples)]\n",
        "\n",
        "        # Shuffle sequences\n",
        "        shuffle(data_strings)\n",
        "\n",
        "        # Cast to numeric each generated binary string\n",
        "        data_x, data_y = [], []\n",
        "        for i in range(num_examples):\n",
        "            train_sequence = []\n",
        "            for binary_char in data_strings[i]:\n",
        "                value = int(binary_char)\n",
        "                train_sequence.append([value])\n",
        "            data_x.append(train_sequence)           # examples are binary sequences of int {0, 1}\n",
        "            data_y.append(np.sum(train_sequence))   # targets are the number of ones in the sequence\n",
        "\n",
        "        # Convert from categorical to one-hot\n",
        "        data_y_one_hot = np.eye(num_classes)[data_y]\n",
        "\n",
        "        # Separate suggested training and test data\n",
        "        train_data      = data_x[:num_train_examples]\n",
        "        train_targets   = data_y_one_hot[:num_train_examples]\n",
        "        test_data       = data_x[num_train_examples:]\n",
        "        test_targets    = data_y_one_hot[num_train_examples:]\n",
        "\n",
        "        return train_data, train_targets, test_data, test_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z54M7WTgzC0m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create and load the sequence dataset"
      ]
    },
    {
      "metadata": {
        "id": "YZKaeAOtvoNK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hidden_size = 40\n",
        "batch_size = 128\n",
        "n_epochs = 128\n",
        "max_sequence_length = 10\n",
        "\n",
        "# Load dataset\n",
        "synthetic_dataset = SyntheticSequenceDataset(max_sequence_length=max_sequence_length)\n",
        "train_data, train_targets, test_data, test_targets = synthetic_dataset.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QSb8xepsybJN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This class represents our recurrent module for learning to count. Your job is to fill in all the methods building the graph for inference, loss function, train step.\n",
        "You may find [tf.contrib.rnn.LSTMCell](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/LSTMCell), [tf.nn.dynamic_rnn](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn), [tf.layers.dense](https://www.tensorflow.org/api_docs/python/tf/layers/dense) useful"
      ]
    },
    {
      "metadata": {
        "id": "kTDWoQaSvdSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define placeholders\n",
        "data = tf.placeholder(shape=(None, max_sequence_length, 1), dtype=tf.float32) # Define the placeholder for the input data sequences\n",
        "targets = tf.placeholder(shape=(None, max_sequence_length+1), dtype=tf.float32) # Define the placeholder for the ground truth\n",
        "\n",
        "class DeepCounter:\n",
        "\n",
        "    def __init__(self, x, targets, hidden_size):\n",
        "\n",
        "        self.x = x\n",
        "        self.targets = targets\n",
        "        self.n_classes = targets.get_shape()[-1]\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.inference = None\n",
        "        self.loss = None\n",
        "        self.train_step = None\n",
        "        self.accuracy = None\n",
        "\n",
        "        self.make_inference()\n",
        "        self.make_loss()\n",
        "        self.make_train_step()\n",
        "        self.make_accuracy()\n",
        "\n",
        "    def make_inference(self):\n",
        "        if self.inference is None:\n",
        "            \"\"\"\n",
        "            TODO:\n",
        "            1) Create a recurrent cell (RNN, LSTM, GRU etc..)\n",
        "            2) Create a recurrent neural network specified by the cell, thus \n",
        "            performing a fully dynamic unrolling of the inputs (stored in self.x)\n",
        "            3) Take the last output of the sequence\n",
        "            4) Append a classification layer.\n",
        "            \"\"\"\n",
        "            cell = tf.nn.rnn_cell.LSTMCell(self.hidden_size)\n",
        "            _, (final_state, _) = tf.nn.dynamic_rnn(cell=cell, inputs=self.x,\n",
        "                                               dtype=tf.float32)\n",
        "            print(final_state)\n",
        "            self.inference = tf.layers.dense(final_state,\n",
        "                                             max_sequence_length + 1,\n",
        "                                             activation=tf.nn.softmax)\n",
        "\n",
        "    def make_loss(self):\n",
        "        if self.loss is None:\n",
        "            self.loss = - tf.reduce_mean(self.targets * tf.log(self.inference + epsilon))\n",
        "\n",
        "    def make_train_step(self):\n",
        "        if self.train_step is None:\n",
        "            optimizer = tf.train.AdamOptimizer()\n",
        "            self.train_step = optimizer.minimize(self.loss)\n",
        "\n",
        "    def make_accuracy(self):\n",
        "        if self.accuracy is None:\n",
        "            correct_predictions = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(self.inference, 1), tf.argmax(self.targets, 1)), tf.float32))\n",
        "            self.accuracy = correct_predictions\n",
        "\n",
        "deep_counter = DeepCounter(x=data, targets=targets, hidden_size=hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xaTvvHcvw_Kn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Training and Test\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "kGtvXoWxziZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Training loop. Your role is to fill the code to run one optimization step"
      ]
    },
    {
      "metadata": {
        "id": "IW1i9m0zwKYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Open session\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initialize variables\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "batches_each_epoch = int(len(train_data)) // batch_size\n",
        "\n",
        "print('\\n' + 50*'*' + '\\nTraining\\n' + 50*'*')\n",
        "\n",
        "# Train batch by batch\n",
        "for epoch in range(0, n_epochs):\n",
        "\n",
        "    start_idx = 0\n",
        "    loss_current_epoch = []\n",
        "    for _ in range(0, batches_each_epoch):\n",
        "\n",
        "        # Load batch\n",
        "        end_idx = start_idx + batch_size\n",
        "        data_batch, target_batch = train_data[start_idx:end_idx], train_targets[start_idx:end_idx]\n",
        "\n",
        "        # Run one optimization step on current step\n",
        "        cur_loss, _ = sess.run([deep_counter.loss, deep_counter.train_step], feed_dict={data: data_batch, targets: target_batch})\n",
        "        loss_current_epoch.append(cur_loss)\n",
        "\n",
        "        # Update data pointer\n",
        "        start_idx += batch_size\n",
        "    \n",
        "    if epoch % 10 == 0:\n",
        "      print('Epoch {:02d} - Loss on train set: {:.02f}'.format(epoch, sum(loss_current_epoch)/batches_each_epoch))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jdXbHggozuFl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run test. Your role is to fill code to run the graph and compute accuracy"
      ]
    },
    {
      "metadata": {
        "id": "jlUYHrCtwMDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('\\n' + 50 * '*' + '\\nTesting\\n' + 50 * '*')\n",
        "\n",
        "accuracy_score = 0.0\n",
        "num_test_batches = int(len(test_data)) // batch_size\n",
        "start_idx = 0\n",
        "\n",
        "# Test batch by batch\n",
        "for _ in range(0, num_test_batches):\n",
        "    end_idx = start_idx + batch_size\n",
        "    data_batch, target_batch = test_data[start_idx:end_idx], test_targets[start_idx:end_idx]\n",
        "    # compute accuracy on batch\n",
        "    accuracy_score += sess.run(deep_counter.accuracy, feed_dict={data: data_batch, targets: target_batch})\n",
        "    start_idx += batch_size\n",
        "\n",
        "print('Average accuracy on test set: {:.03f}'.format(accuracy_score / num_test_batches))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DvF2hegY0Bqa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Interactive section, just try to stress the model typing sequences yourself! :)"
      ]
    },
    {
      "metadata": {
        "id": "l6H0Fu-UwPu2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('\\n' + 50 * '*' + '\\nInteractive Session\\n' + 50 * '*')\n",
        "\n",
        "while True:\n",
        "    my_sequence = raw_input('Write your own binary sequence 10 digits in {0, 1}:\\n')\n",
        "    if my_sequence:\n",
        "\n",
        "        # Pad shorter sequences\n",
        "        if len(my_sequence) < max_sequence_length:\n",
        "            my_sequence = (max_sequence_length - len(my_sequence))*'0' + my_sequence\n",
        "\n",
        "        # Crop longer sequences\n",
        "        my_sequence = my_sequence[:max_sequence_length]\n",
        "\n",
        "        # Prepare example\n",
        "        test_example = []\n",
        "        for binary_char in my_sequence:\n",
        "            test_example.append([float(binary_char)])\n",
        "\n",
        "        pred = sess.run(deep_counter.inference, feed_dict={data: np.expand_dims(test_example, 0)})\n",
        "        print('Predicted number of ones: {} - Real: {}\\n'.format(int(np.argmax(pred)), int(np.sum(test_example))))\n",
        "    else:\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}